{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import pipeline,preprocessing,feature_extraction,metrics\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('russian')\n",
    "\n",
    "from sklearn.base import BaseEstimator \n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.pipeline  import FeatureUnion\n",
    "from sklearn.pipeline  import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TfIdf\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import xgboost as xgb\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"~/Kaggle/avito/train.csv\")\n",
    "test_df = pd.read_csv(\"~/Kaggle/avito/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "def count_punctuation(aStr):\n",
    "    return count(aStr,set(string.punctuation))\n",
    "\n",
    "\n",
    "def reduce_category(seriesOld, min_percentage=0.03, new_cat='other'):\n",
    "    series = seriesOld.copy()\n",
    "    value_counts = series.value_counts()\n",
    "    count = series.count()\n",
    "    min_count = count * min_percentage\n",
    "\n",
    "    select = series.apply(lambda val: not pd.isnull(val) and value_counts.get(val) < min_count)\n",
    "    series.loc[select] = new_cat\n",
    "    return series\n",
    "\n",
    "def preprocess(dfOld: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = dfOld.copy()\n",
    "    \n",
    "    #df['description'] = dfOld['description'].replace(r\"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()]\", \" \")\n",
    "    #df['city'] = dfOld['city'].replace(r\"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()]\", \" \")\n",
    "    \n",
    "     # isna feature\n",
    "    #print(pd.isna(dfOld['param_3']))\n",
    "    df['param_3_na'] = pd.isna(dfOld['param_3']).astype(np.float16)\n",
    "    df['param_2_na'] = pd.isna(dfOld['param_2']).astype(np.float16)\n",
    "    df['description_na'] = pd.isna(dfOld['description']).astype(np.float16)\n",
    "    df['image_na'] = pd.isna(dfOld['image']).astype(np.float16)\n",
    "    \n",
    "    # description and title statistics\n",
    "    df['description_len'] = dfOld['description'].fillna('').map(lambda x: len(str(x))).astype(np.float16) #Length\n",
    "    df['description_wc'] = dfOld['description'].fillna('').map(lambda x: len(str(x).split(' '))).astype(np.float16) #Word Count\n",
    "    df['description_punc'] = dfOld['description'].fillna('').map(lambda x: count_punctuation(x)).astype(np.float16) #punctuation Count\n",
    "    df['description_sw'] = dfOld['description'].fillna('').map(lambda x: str(x).split(' ')).map(lambda aStr: count(aStr,set(sw))).astype(np.float16) # stopwords\n",
    "    \n",
    "    df['title_len'] = dfOld['title'].fillna('').map(lambda x: len(str(x))).astype(np.float16) #Lenth\n",
    "    df['title_wc'] = dfOld['title'].fillna('').map(lambda x: len(str(x).split(' '))).astype(np.float16) #Word Count\n",
    "    df['title_punc'] = dfOld['title'].fillna('').map(lambda x: count_punctuation(x)).astype(np.float16) #punctuation Count\n",
    "    \n",
    "    # description feature for tdidf\n",
    "    df['description'] = (dfOld['title'].fillna('') + ' ' + dfOld['description'].fillna(''))\n",
    "    df['description'] = df['description'].str.lower().replace(r\"[^[:alpha:]]\", \" \")\n",
    "    df['description'] = df['description'].str.replace(r\"\\\\s+\", \" \")\n",
    "    \n",
    "    #reduce category\n",
    "    df['param_3'] = reduce_category(df['param_3'], min_percentage=0.00005, new_cat='other_param')\n",
    "    df['city'] = reduce_category(dfOld['city'], min_percentage=0.0003, new_cat='other_city')\n",
    "    df['user_id'] = reduce_category(dfOld['user_id'], min_percentage=0.000025, new_cat='other_user')\n",
    "    \n",
    "    df['image'] = df['image'].fillna('').map(lambda x: 1 if len(str(x))>0 else 0)\n",
    "    df['price'] = np.log1p(df['price'].fillna(0))\n",
    "    df['image_top_1'] = np.log1p(df['image_top_1'].fillna(0))\n",
    "    \n",
    "    # extract activation date\n",
    "    df['activation_date_wday'] = pd.to_datetime(df['activation_date']).dt.dayofweek\n",
    "    df['activation_date_day'] = pd.to_datetime(df['activation_date']).dt.day\n",
    "    df['activation_date_week'] = pd.to_datetime(df['activation_date']).dt.week\n",
    "    \n",
    "    #ex_col = ['item_id', 'user_id', 'deal_probability', 'title', 'param_1', 'param_2', 'param_3', 'activation_date', 'city']\n",
    "    ex_col = ['item_id', 'deal_probability', 'title', 'activation_date']\n",
    "    col = [c for c in df.columns if c not in ex_col]\n",
    "    return df[col]\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        return X[self.key]\n",
    "    \n",
    "class FeatureInfo(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        #print('FeatureInfo ' + str(type(X)))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print('FeatureInfo ' + str(X.shape) + str(type(X)))\n",
    "        return X\n",
    "    \n",
    "class ArrayCaster(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        #print(data.shape)\n",
    "        #print(np.transpose(np.matrix(data)).shape)\n",
    "        return np.transpose(np.matrix(data))\n",
    "    \n",
    "class NaFiller(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, filler):\n",
    "        self.filler = filler\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return pd.Series(data).fillna(-1)\n",
    "\n",
    "class MyLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        unique = pd.Series(X).unique()\n",
    "        self.unique_values  = unique[pd.notnull(unique)].tolist()\n",
    "        #print(self.unique_values)\n",
    "        return self\n",
    "    def transform(self, dataOld):\n",
    "        data = pd.Series(dataOld)\n",
    "        mapper = lambda datum: self.unique_values.index(datum) if datum in self.unique_values else -1\n",
    "        return data.map(mapper)\n",
    "    \n",
    "class LabelEncoders(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, id):\n",
    "        self.mappings = {}\n",
    "        self.id = id\n",
    "    def fit(self, X, y=None):\n",
    "        name = X.name\n",
    "        #print(\"MyLabelEncoders \" + name + self.id)\n",
    "        if (name not in self.mappings):\n",
    "            self.mappings[name] = MyLabelEncoder()\n",
    "        self.mappings[name].fit(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        result = self.mappings[X.name].transform(X)\n",
    "        #print(\"LabelEncoders\" + str(result.shape))\n",
    "        return result\n",
    "    \n",
    "def on_field(key:str, *transformers) -> Pipeline:\n",
    "    additional_steps = [ (\"step\" + str(ii), transformers[ii]) for ii in range(len(transformers)) ]\n",
    "    steps =  [('selection', FeatureSelector(key=key)) ] + additional_steps\n",
    "    print(steps)\n",
    "    return Pipeline(steps)\n",
    "\n",
    "def on_multiple_fields(keys, *transformers) -> Pipeline:\n",
    "    return FeatureUnion(\n",
    "        transformer_list=[\n",
    "            (\"pipeline\" + str(keys[ii]), on_field(keys[ii], *transformers)) for ii in range(len(keys))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Big data experiment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# shuffle\n",
    "def split_train_valid_dataset(old_train_df, dealProb):\n",
    "    ss = ShuffleSplit(n_splits=1, test_size=0.1, random_state=37)\n",
    "    train_index, test_index = next(ss.split(train_df))\n",
    "\n",
    "    train_X = old_train_df.iloc[train_index]\n",
    "    train_Y = dealProb[train_index]\n",
    "\n",
    "    valid_X = old_train_df.iloc[test_index]\n",
    "    valid_Y = dealProb[test_index]\n",
    "    \n",
    "    return (train_X, train_Y, valid_X, valid_Y)\n",
    "\n",
    "\n",
    "train_df_prep = preprocess(train_df)\n",
    "deal_prob = train_df['deal_probability']\n",
    "(train_X, train_Y, valid_X, valid_Y) = split_train_valid_dataset(train_df_prep, deal_prob)\n",
    "#train_X_prep = preprocess(train_X)\n",
    "#train_Y_prep = preprocess(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                                                      d9b494f97062\n",
       "region                                                Воронежская область\n",
       "city                                                              Воронеж\n",
       "parent_category_name                                          Личные вещи\n",
       "category_name                                  Товары для детей и игрушки\n",
       "param_1                                                           Игрушки\n",
       "param_2                                                               NaN\n",
       "param_3                                                               NaN\n",
       "description             пинки пай my little pony пинки пай ждёт любящу...\n",
       "price                                                             7.31389\n",
       "item_seq_number                                                       309\n",
       "user_type                                                         Company\n",
       "image                                                                   1\n",
       "image_top_1                                                       6.86485\n",
       "param_3_na                                                              1\n",
       "param_2_na                                                              1\n",
       "description_na                                                          0\n",
       "image_na                                                                0\n",
       "description_len                                                       210\n",
       "description_wc                                                         31\n",
       "description_punc                                                       12\n",
       "description_sw                                                         10\n",
       "title_len                                                              24\n",
       "title_wc                                                                5\n",
       "title_punc                                                              0\n",
       "activation_date_wday                                                    4\n",
       "activation_date_day                                                    24\n",
       "activation_date_week                                                   12\n",
       "Name: 25, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('selection', FeatureSelector(key='description')), ('step0', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.3, max_features=3500, min_df=3,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', '...гда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'],\n",
      "        strip_accents=None, sublinear_tf=True, token_pattern='\\\\w+',\n",
      "        tokenizer=None, use_idf=True, vocabulary=None))]\n",
      "[('selection', FeatureSelector(key='region')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='parent_category_name')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='category_name')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='user_type')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='param_3')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='param_2')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='param_1')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='city')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='user_id')), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='item_seq_number')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='param_3_na')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='param_2_na')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='description_na')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='image_na')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='description_len')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='description_wc')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='description_punc')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='description_sw')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='title_len')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='title_wc')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='title_punc')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='price')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='image_top_1')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='activation_date_wday')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='activation_date_day')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='activation_date_week')), ('step0', NaFiller(filler=-1)), ('step1', ArrayCaster())]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = pipeline.make_pipeline(\n",
    "                pipeline.make_union(\n",
    "                    on_field('description', TfIdf(max_features=3500, stop_words=sw, token_pattern='\\w+', norm='l2', min_df=3, max_df=0.3, sublinear_tf=True, ngram_range=(1, 3))),\n",
    "                    on_multiple_fields(['region', 'parent_category_name', 'category_name', 'user_type', 'param_3', 'param_2', 'param_1', 'city', 'user_id'], LabelEncoders(id=\"same\"), ArrayCaster()),\n",
    "                    on_multiple_fields(['item_seq_number', 'param_3_na', 'param_2_na', 'description_na', 'image_na', 'description_len', 'description_wc', 'description_punc', 'description_sw', 'title_len',  'title_wc', 'title_punc', 'price', 'image_top_1', 'activation_date_wday', 'activation_date_day', 'activation_date_week'], NaFiller(-1), ArrayCaster())\n",
    "                ),\n",
    "                #RandomForestRegressor()\n",
    ")\n",
    "\n",
    "vectorizer.fit(train_X)\n",
    "train_X_transform = vectorizer.transform(train_X)\n",
    "valid_X_transform = vectorizer.transform(valid_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test with randomforestregressor</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test with xgboost </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=0, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.7, eta=0.05, eval='rmse', gamma=0, lambda=2,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=18,\n",
       "       min_child_weight=11, missing=None, n_estimators=100, n_jobs=16,\n",
       "       nrounds=8000, nthread=None, objective='reg:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=37, silent=False, subsample=0.8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import xgboost\n",
    "\n",
    "train_dmatrix = xgboost.DMatrix(data=train_X_transform, label=train_Y)\n",
    "valid_dmatrix = xgboost.DMatrix(data=valid_X_transform, label=valid_Y)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval': 'rmse',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 18,\n",
    "    'min_child_weight': 11,\n",
    "    'gamma': 0,\n",
    "    'subsample': 0.8,\n",
    "    'alpha': 0,\n",
    "    'lambda': 2,\n",
    "    'nrounds': 8000,\n",
    "    'colsample_bytree': 0.7\n",
    "}\n",
    "\n",
    "#xgbclf = xgboost.train(params, train_dmatrix, early_stopping_rounds=50)\n",
    "xgbclf = XGBRegressor(n_jobs=16, \n",
    "                      silent=False,\n",
    "                      seed=37,\n",
    "                      **params\n",
    "                     )\n",
    "xgbclf.fit(train_X_transform, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20119690964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20119690964"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def count_mse(xgbclf, valid_X, valid_Y):\n",
    "    valid_X_pred = xgbclf.predict(valid_X)\n",
    "    error = mse(valid_Y, valid_X_pred)\n",
    "    print(error)\n",
    "    return error\n",
    "count_mse(xgbclf, valid_X_transform, valid_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test with xgboost </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1503424, 7015)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "xgbclf2 = XGBRegressor(n_jobs=16, silent=False)\n",
    "all_X =  sp.vstack((train_X_t, test_X_t))\n",
    "all_Y = sp.hstack((train_Y, test_Y)).T\n",
    "all_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1503424, 1)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2cddbb0956b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgbclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    261\u001b[0m                                    missing=self.missing, nthread=self.n_jobs)\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mtrainDmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mevals_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                                      ctypes.byref(self.handle)))\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_csr\u001b[0;34m(self, csr)\u001b[0m\n\u001b[1;32m    306\u001b[0m         _check_call(_LIB.XGDMatrixCreateFromCSREx(c_array(ctypes.c_size_t, csr.indptr),\n\u001b[1;32m    307\u001b[0m                                                   \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_uint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                                                   \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m                                                   \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                                   \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(all_Y.shape)\n",
    "xgbclf2.fit(all_X, all_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "test_df_prep = preprocess(test_df)\n",
    "test_df_prep_t = vectorizer.transform(test_df_prep)\n",
    "test_df_prep_t_pred = xgbclf.predict(test_df_prep_t)\n",
    "\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['item_id'] = test_df['item_id']\n",
    "df_submission['deal_probability'] = pd.Series(test_df_prep_t_pred)\n",
    "\n",
    "df_submission.to_csv('submission.csv', index=False, index_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_prep_t_pred_series = pd.Series(test_df_prep_t_pred)\n",
    "minn = test_df_prep_t_pred_series.min()\n",
    "maxx = test_df_prep_t_pred_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_prep_t_pred_series=test_df_prep_t_pred_series.apply(lambda x: (x-minn)/(maxx-minn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    508438.000000\n",
       "mean          0.258402\n",
       "std           0.136093\n",
       "min           0.000000\n",
       "25%           0.140774\n",
       "50%           0.240963\n",
       "75%           0.323774\n",
       "max           1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_prep_t_pred_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame()\n",
    "df_submission['item_id'] = test_df['item_id']\n",
    "df_submission['deal_probability'] = test_df_prep_t_pred_series\n",
    "\n",
    "df_submission.to_csv('submission.csv', index=False, index_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    508438.000000\n",
       "mean          0.258402\n",
       "std           0.136093\n",
       "min           0.000000\n",
       "25%           0.140774\n",
       "50%           0.240963\n",
       "75%           0.323774\n",
       "max           1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_prep_t_pred_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    508438.000000\n",
       "mean          0.146537\n",
       "std           0.095903\n",
       "min          -0.035600\n",
       "25%           0.063625\n",
       "50%           0.134243\n",
       "75%           0.192612\n",
       "max           0.669253\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_df_prep_t_pred).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
