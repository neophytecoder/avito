{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import pipeline,preprocessing,feature_extraction,metrics\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('russian')\n",
    "\n",
    "from sklearn.base import BaseEstimator \n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.pipeline  import FeatureUnion\n",
    "from sklearn.pipeline  import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TfIdf\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import xgboost as xgb\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"~/Kaggle/avito/train.csv\", nrows=50)\n",
    "#test_df = pd.read_csv(\"../test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "def count_punctuation(aStr):\n",
    "    return count(aStr,set(string.punctuation))\n",
    "\n",
    "\n",
    "def reduce_category(seriesOld, min_percentage=0.03, new_cat='other'):\n",
    "    series = seriesOld.copy()\n",
    "    value_counts = series.value_counts()\n",
    "    count = series.fillna('').count()\n",
    "    min_count = count * min_percentage\n",
    "\n",
    "    select = series.apply(lambda val: not pd.isnull(val) and value_counts.get(val) < min_count)\n",
    "    series.loc[select] = new_cat\n",
    "    return series\n",
    "\n",
    "def preprocess(dfOld: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = dfOld.copy()\n",
    "    \n",
    "    #df['description'] = dfOld['description'].replace(r\"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()]\", \" \")\n",
    "    df['city'] = dfOld['city'].replace(r\"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()]\", \" \")\n",
    "    \n",
    "     # isna feature\n",
    "    df['param_3_na'] = pd.isna(dfOld['param_3']).astype(np.float16)\n",
    "    df['param_2_na'] = pd.isna(dfOld['param_2']).astype(np.float16)\n",
    "    df['description_na'] = pd.isna(dfOld['description']).astype(np.float16)\n",
    "    df['image_na'] = pd.isna(dfOld['image']).astype(np.float16)\n",
    "    \n",
    "    # description and title statistics\n",
    "    df['description_len'] = df['description'].fillna('').map(lambda x: len(str(x))).astype(np.float16) #Length\n",
    "    df['description_wc'] = df['description'].fillna('').map(lambda x: len(str(x).split(' '))).astype(np.float16) #Word Count\n",
    "    df['description_punc'] = df['description'].fillna('').map(lambda x: count_punctuation(x)).astype(np.float16) #punctuation Count\n",
    "    df['description_sw'] = df['description'].fillna('').map(lambda x: str(x).split(' ')).map(lambda aStr: count(aStr,set(sw))).astype(np.float16) # stopwords\n",
    "    \n",
    "    df['title_len'] = df['title'].fillna('').map(lambda x: len(str(x))).astype(np.float16) #Lenth\n",
    "    df['title_wc'] = df['title'].fillna('').map(lambda x: len(str(x).split(' '))).astype(np.float16) #Word Count\n",
    "    df['title_punc'] = df['title'].fillna('').map(lambda x: count_punctuation(x)).astype(np.float16) #punctuation Count\n",
    "    \n",
    "    # description feature for tdidf\n",
    "    df['description'] = (df['title'].fillna('') + ' ' + df['description'].fillna(''))\n",
    "    df['description'] = df['description'].str.lower().replace(r\"[^[:alpha:]]\", \" \")\n",
    "    df['description'] = df['description'].str.replace(r\"\\\\s+\", \" \")\n",
    "    \n",
    "    #reduce category\n",
    "    df['param_3'] = reduce_category(df['param_3'], min_percentage=0.00005, new_cat='other_param')\n",
    "    df['city'] = reduce_category(df['city'], min_percentage=0.0003, new_cat='other_city')\n",
    "    df['user_id'] = reduce_category(df['user_id'], min_percentage=0.000025, new_cat='other_user')\n",
    "    \n",
    "    df['image'] = df['image'].fillna('').map(lambda x: 1 if len(str(x))>0 else 0)\n",
    "    df['price'] = np.log1p(df['price'].fillna(0))\n",
    "    df['image_top_1'] = np.log1p(df['image_top_1'].fillna(0))\n",
    "    \n",
    "    # extract activation date\n",
    "    df['activation_date_wday'] = pd.to_datetime(df['activation_date']).dt.dayofweek\n",
    "    df['activation_date_day'] = pd.to_datetime(df['activation_date']).dt.day\n",
    "    df['activation_date_week'] = pd.to_datetime(df['activation_date']).dt.week\n",
    "    \n",
    "    ex_col = ['item_id', 'user_id', 'deal_probability', 'title', 'param_1', 'param_2', 'param_3', 'activation_date', 'city']\n",
    "    col = [c for c in df.columns if c not in ex_col]\n",
    "    return df[col]\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        return X[self.key]\n",
    "    \n",
    "class FeatureInfo(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        #print('FeatureInfo ' + str(type(X)))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print('FeatureInfo ' + str(X.shape) + str(type(X)))\n",
    "        return X\n",
    "    \n",
    "class ArrayCaster(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        #print(data.shape)\n",
    "        #print(np.transpose(np.matrix(data)).shape)\n",
    "        return np.transpose(np.matrix(data))\n",
    "\n",
    "    \n",
    "class LabelEncoders(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, id):\n",
    "        self.mappings = {}\n",
    "        self.id = id\n",
    "    def fit(self, X, y=None):\n",
    "        name = X.name\n",
    "        print(\"LabelEncoders \" + name + self.id)\n",
    "        if (name not in self.mappings):\n",
    "            self.mappings[name] = preprocessing.LabelEncoder()\n",
    "        self.mappings[name].fit(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        result = self.mappings[X.name].transform(X)\n",
    "        print(\"LabelEncoders\" + str(result.shape))\n",
    "        return result\n",
    "    \n",
    "def on_field(key:str, *transformers) -> Pipeline:\n",
    "    additional_steps = [ (\"step\" + str(ii), transformers[ii]) for ii in range(len(transformers)) ]\n",
    "    steps =  [('selection', FeatureSelector(key=key)),  ('info', FeatureInfo()) ] + additional_steps\n",
    "    print(steps)\n",
    "    return Pipeline(steps)\n",
    "\n",
    "def on_multiple_fields(keys, *transformers) -> Pipeline:\n",
    "    return FeatureUnion(\n",
    "        transformer_list=[\n",
    "            (\"pipeline\" + str(keys[ii]), on_field(keys[ii], *transformers)) for ii in range(len(keys))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Big data experiment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('selection', FeatureSelector(key='description')), ('info', FeatureInfo()), ('step0', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.3, max_features=3500, min_df=3,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', '...гда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'],\n",
      "        strip_accents=None, sublinear_tf=True, token_pattern='\\\\w+',\n",
      "        tokenizer=None, use_idf=True, vocabulary=None))]\n",
      "[('selection', FeatureSelector(key='param')), ('info', FeatureInfo()), ('step0', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.3, max_features=3500, min_df=3,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', '...гда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'],\n",
      "        strip_accents=None, sublinear_tf=True, token_pattern='\\\\w+',\n",
      "        tokenizer=None, use_idf=True, vocabulary=None))]\n",
      "[('selection', FeatureSelector(key='region')), ('info', FeatureInfo()), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='parent_category_name')), ('info', FeatureInfo()), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='category_name')), ('info', FeatureInfo()), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='user_type')), ('info', FeatureInfo()), ('step0', LabelEncoders(id='same')), ('step1', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='price')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='item_seq_number')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='image')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='image_top_1')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='description_len')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='description_wc')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='title_len')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='title_wc')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='wday')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='day')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n",
      "[('selection', FeatureSelector(key='week')), ('info', FeatureInfo()), ('step0', ArrayCaster())]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = pipeline.make_pipeline(\n",
    "                pipeline.make_union(\n",
    "                    on_field('description', TfIdf(max_features=3500, stop_words=sw, token_pattern='\\w+', norm='l2', min_df=3, max_df=0.3, sublinear_tf=True, ngram_range=(1, 3))),\n",
    "                    on_field('param', TfIdf(max_features=3500, stop_words=sw, token_pattern='\\w+', norm='l2',min_df=3, max_df=0.3, sublinear_tf=True, ngram_range=(1, 3))),\n",
    "                    on_multiple_fields(['region', 'parent_category_name', 'category_name', 'user_type'], LabelEncoders(id=\"same\"), ArrayCaster()),\n",
    "                    on_multiple_fields(['price', 'item_seq_number', 'image', 'image_top_1', 'description_len', 'description_wc', 'title_len', 'title_wc', 'wday', 'day', 'week'], ArrayCaster())\n",
    "                ),\n",
    "                #RandomForestRegressor()\n",
    ")\n",
    "\n",
    "# shuffle\n",
    "ss = ShuffleSplit(n_splits=1, test_size=0.2, random_state=37)\n",
    "train_index, test_index = next(ss.split(train_df))\n",
    "\n",
    "train_df_prep = preprocess(train_df)\n",
    "train_df_Y = train_df['deal_probability']\n",
    "\n",
    "train_X = train_df_prep.iloc[train_index]\n",
    "train_Y = train_df['deal_probability'][train_index]\n",
    "\n",
    "test_X = train_df_prep.iloc[test_index]\n",
    "test_Y = train_df['deal_probability'][test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders regionsame\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders parent_category_namesame\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders category_namesame\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders user_typesame\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "     steps=[('selection', FeatureSelector(key='description')), ('info', FeatureInfo()), ('step0', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "     ...='week')), ('info', FeatureInfo()), ('step0', ArrayCaster())]))],\n",
       "       transformer_weights=None))],\n",
       "       transformer_weights=None))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test with randomforestregressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(1202739,)\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (1202739,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(300685,)\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(300685,)\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(300685,)\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(300685,)\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (300685,)<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8096aeb6f04c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_X_t = vectorizer.transform(train_X)\n",
    "test_X_t = vectorizer.transform(test_X)\n",
    "\n",
    "clf = RandomForestRegressor(n_jobs=16)\n",
    "clf.fit(train_X_t, train_Y)\n",
    "\n",
    "test_pred = clf.predict(test_X_t)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "error = mse(test_Y, test_pred)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test with xgboost </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f80d37440882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgbclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_pred_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf2' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgbclf = XGBRegressor(n_jobs=16, silent=False)\n",
    "xgbclf.fit(train_X_t, train_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05549476162360624\n"
     ]
    }
   ],
   "source": [
    "test_pred_xgb = xgbclf.predict(test_X_t)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "error = mse(test_Y, test_pred_xgb)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test with xgboost </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1503424, 7015)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "xgbclf2 = XGBRegressor(n_jobs=16, silent=False)\n",
    "all_X =  sp.vstack((train_X_t, test_X_t))\n",
    "all_Y = sp.hstack((train_Y, test_Y)).T\n",
    "all_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1503424, 1)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2cddbb0956b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgbclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    261\u001b[0m                                    missing=self.missing, nthread=self.n_jobs)\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mtrainDmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mevals_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                                      ctypes.byref(self.handle)))\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_csr\u001b[0;34m(self, csr)\u001b[0m\n\u001b[1;32m    306\u001b[0m         _check_call(_LIB.XGDMatrixCreateFromCSREx(c_array(ctypes.c_size_t, csr.indptr),\n\u001b[1;32m    307\u001b[0m                                                   \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_uint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                                                   \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m                                                   \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                                   \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(all_Y.shape)\n",
    "xgbclf2.fit(all_X, all_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "LabelEncoders(508438,)\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n",
      "FeatureInfo (508438,)<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "test_df_prep = preprocess(test_df)\n",
    "test_df_prep_t = vectorizer.transform(test_df_prep)\n",
    "test_df_prep_t_pred = xgbclf.predict(test_df_prep_t)\n",
    "\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['item_id'] = test_df['item_id']\n",
    "df_submission['deal_probability'] = pd.Series(test_df_prep_t_pred)\n",
    "\n",
    "df_submission.to_csv('submission.csv', index=False, index_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_prep_t_pred_series = pd.Series(test_df_prep_t_pred)\n",
    "minn = test_df_prep_t_pred_series.min()\n",
    "maxx = test_df_prep_t_pred_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_prep_t_pred_series=test_df_prep_t_pred_series.apply(lambda x: (x-minn)/(maxx-minn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    508438.000000\n",
       "mean          0.258402\n",
       "std           0.136093\n",
       "min           0.000000\n",
       "25%           0.140774\n",
       "50%           0.240963\n",
       "75%           0.323774\n",
       "max           1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_prep_t_pred_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame()\n",
    "df_submission['item_id'] = test_df['item_id']\n",
    "df_submission['deal_probability'] = test_df_prep_t_pred_series\n",
    "\n",
    "df_submission.to_csv('submission.csv', index=False, index_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    508438.000000\n",
       "mean          0.258402\n",
       "std           0.136093\n",
       "min           0.000000\n",
       "25%           0.140774\n",
       "50%           0.240963\n",
       "75%           0.323774\n",
       "max           1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_prep_t_pred_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    508438.000000\n",
       "mean          0.146537\n",
       "std           0.095903\n",
       "min          -0.035600\n",
       "25%           0.063625\n",
       "50%           0.134243\n",
       "75%           0.192612\n",
       "max           0.669253\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_df_prep_t_pred).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
